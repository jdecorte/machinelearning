{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/jdecorte/machinelearning/blob/main/131-pretrained_word_embeddings_case_spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAMKfJvV85iq"
      },
      "source": [
        "\n",
        "## Using pre-trained word embeddings\n",
        "\n",
        "### Introduction\n",
        "Perform spam classification.  \n",
        "\n",
        "The data can be downloaded from UCI machine learning repository: https://archive.ics.uci.edu/ml/machine-learning-databases/00228 \n",
        "Information about the dataset can be found at   https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "\n",
        "*   A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages.\n",
        "*   A subset of 3,375 SMSâ€™s randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available.\n",
        "*   A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis.\n",
        "*   Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available.\n",
        "\n",
        "**Attribute Information:**\n",
        "The collection is composed by just one text file, where each line has the correct class followed by the raw message. The messages are not chronologically sorted. \n",
        "\n",
        "Examples:\n",
        "\n",
        "spam I am not spam. subscribe to win money\n",
        "\n",
        "ham Mary had a little lamb\n",
        "\n",
        "Here are some examples bellow: \n",
        "\n",
        "ham What you doing?how are you? \n",
        "\n",
        "ham Ok lar... Joking wif u oni... \n",
        "\n",
        "ham dun say so early hor... U c already then say... \n",
        "\n",
        "ham MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H* \n",
        "\n",
        "ham Siva is in hostel aha:-. \n",
        "\n",
        "ham Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\n",
        "\n",
        "spam FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop \n",
        "spam Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B \n",
        "\n",
        "spam URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpMifYkx85is",
        "outputId": "7bcf8530-07df-433f-b478-895c35043087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "import pandas as pd\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 2020\n",
        "np.random.seed(seed)  \n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Embedding, Conv1D,  MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Ms0fWnI2PyL2"
      },
      "outputs": [],
      "source": [
        "# helper functions for visualisation\n",
        "# plotting the loss functions used in this notebook\n",
        "# we plot the loss we want to optimise on the left (in this case: accuracy)\n",
        "def plot_history(history):\n",
        "  plt.figure(figsize = (12,4))\n",
        "  plt.subplot(1,2,1)\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(history.epoch, np.array(history.history['accuracy']),'g-',\n",
        "           label='Train accuracy')\n",
        "  plt.plot(history.epoch, np.array(history.history['val_accuracy']),'r-',\n",
        "           label = 'Validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss minimised by model')\n",
        "  plt.plot(history.epoch, np.array(history.history['loss']),'g-',\n",
        "           label='Train loss')\n",
        "  plt.plot(history.epoch, np.array(history.history['val_loss']),'r-',\n",
        "           label = 'Validation loss')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-FrOQlg_Z-D",
        "outputId": "71a67923-173f-47e0-f101-a6135c7167ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label    object\n",
              "text     object\n",
              "dtype: object"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dataset = pd.read_csv(\"https://raw.githubusercontent.com/jdecorte/machinelearning/main/datasets/SMSSpamCollection.csv\") \n",
        "df_dataset.columns\n",
        "df_dataset.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ljOBisG4Eju7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Take a look at the data\n",
        "df_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "J8Jl5bZZRIbF",
        "outputId": "4d75789b-51f5-4905-8765-2ea4f2d22358"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      1  Go until jurong point, crazy.. Available only ...\n",
              "1      1                      Ok lar... Joking wif u oni...\n",
              "2      0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      1  U dun say so early hor... U c already then say...\n",
              "4      1  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Changing spam and ham into 0 and 1\n",
        "df_dataset['label'] = np.where(df_dataset['label'] == \"spam\", 0, 1)\n",
        "df_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8jeo1-3RGIFC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5572.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.865937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.340751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             label\n",
              "count  5572.000000\n",
              "mean      0.865937\n",
              "std       0.340751\n",
              "min       0.000000\n",
              "25%       1.000000\n",
              "50%       1.000000\n",
              "75%       1.000000\n",
              "max       1.000000"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the general information about the data\n",
        "df_dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "jpTHHeWgG0ce",
        "outputId": "f0e187bf-7d79-4682-aeca-4646f716d940"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>numberOfWords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text  numberOfWords\n",
              "0      1  Go until jurong point, crazy.. Available only ...             20\n",
              "1      1                      Ok lar... Joking wif u oni...              6\n",
              "2      0  Free entry in 2 a wkly comp to win FA Cup fina...             28\n",
              "3      1  U dun say so early hor... U c already then say...             11\n",
              "4      1  Nah I don't think he goes to usf, he lives aro...             13"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# What is the average length of the SMS messages. This will be important when we need to truncate the sequences to a maximum length\n",
        "df_dataset['numberOfWords'] = df_dataset.text.str.split().apply(len)\n",
        "df_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "l-4cl8dqHUyU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    5572.000000\n",
              "mean       15.597452\n",
              "std        11.404053\n",
              "min         1.000000\n",
              "25%         7.000000\n",
              "50%        12.000000\n",
              "75%        23.000000\n",
              "max       171.000000\n",
              "Name: numberOfWords, dtype: float64"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the general information about the column numberOfWords\n",
        "# Notice that 75% of the messages consists of only 23 words or less.\n",
        "df_dataset['numberOfWords'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK5zHfTOBQs7",
        "outputId": "57cd2aea-6d28-4326-89a0-129484b9454b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (3900, 1)\n",
            "X_test shape: (1672, 1)\n",
            "3900 train samples\n",
            "1672 test samples\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "# Extract a training & validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df_dataset.drop(['label','numberOfWords'],axis=1)\n",
        "y = df_dataset['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "print(type(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsAXPQD9cViO",
        "outputId": "86a329f2-3484-42f4-af63-d44b7c1b8205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (3900, 1)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "X_train = np.asarray(X_train)\n",
        "X_test = np.asarray(X_test)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(type(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YNIcfYeQ8QB",
        "outputId": "f1140ff9-4550-47a4-9333-55dc2ee0c0e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "y_train shape: (3900,)\n",
            "y_test shape: (1672,)\n"
          ]
        }
      ],
      "source": [
        "# look at the new labels for the first sample\n",
        "print(y_train[0])\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSvPJXT0C0Q8"
      },
      "source": [
        "### Create a vocabulary index\n",
        "\n",
        "Let's use the TextVectorization to index the vocabulary found in the dataset. \n",
        "\n",
        "Deep Learning systems are often trained on very large datasets that will not fit in RAM. Ingesting a large dataset and preprocessing it efficiently can be tricky to implement with other Deep Learning libraries, but TensorFlow (on which Keras is based) makes it easy thanks to the Data API: you just create a **dataset** object, and tell it where to get the data and how to transform it. TensorFlow takes care of all the implementation details, such as multithreading, queuing, batching, and prefetching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "t9vR-3X7Cy7D"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# max_tokens = integer parameter that will control the maximum size of the vocabulary. \n",
        "# We will only consider the top 20 000 words\n",
        "# output_sequence_length = 25: we will truncate or pad sequences to be actually 25 tokens long.\n",
        "# This is the reason why we calculated the number of words for each message in a previouse step\n",
        "# From the describe we learn that 75% of the messages are not longer than 23 tokens\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=25)\n",
        "\n",
        "# Make a Dataset from a numpy array\n",
        "# A tf.data.Dataset represents a potentially large set of elements.\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "\n",
        "# Call the adapt method to build the vocabulary\n",
        "vectorizer.adapt(text_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQx8k5FECtSU",
        "outputId": "19546e91-db30-4d97-ce8c-8e8446292895"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'i',\n",
              " 'to',\n",
              " 'you',\n",
              " 'a',\n",
              " 'the',\n",
              " 'u',\n",
              " 'and',\n",
              " 'is',\n",
              " 'in',\n",
              " 'my',\n",
              " 'me',\n",
              " 'for',\n",
              " 'your',\n",
              " 'of',\n",
              " 'it',\n",
              " 'have',\n",
              " 'call',\n",
              " 'on',\n",
              " 'that',\n",
              " 'are',\n",
              " 'now',\n",
              " '2',\n",
              " 'im']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can retrieve the computed vocabulary used via vectorizer.get_vocabulary(). \n",
        "# Let's print the top 25 words:\n",
        "vectorizer.get_vocabulary()[:25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrXr5Ry8Dn_j",
        "outputId": "f9d1387c-8a25-45b4-b449-838f4ca37ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([   2,  529,    6, 2136,  422,   19,    6, 5450], dtype=int64)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's vectorize a test sentence, based on the vocabulary we created from the training dataset):\n",
        "output = vectorizer([[\"i saw the cat sat on the mat\"]])\n",
        "print(type(output))\n",
        "output.numpy()[0, :8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Iuzq59D-fb"
      },
      "source": [
        "As you can see, \"i\" gets represented as \"2\". Why not 0, given that \"i\" was the second word in the vocabulary? That's because index 0 is reserved for the padding token and index 1 is reserved for \"out of vocabulary\" tokens.\n",
        "\n",
        "Here's a dictionary mapping words of the vocabulary of the dataset to their indices in the :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "cYtTe3gQDwEA"
      },
      "outputs": [],
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Eax6cpESR2"
      },
      "source": [
        "As you can see, we obtain the same encoding as above for our test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHO0wHTTENcE",
        "outputId": "ebc75077-2dc4-43b1-bced-0626762e27b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 529, 6, 2136, 422, 19, 6, 5450]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = [\"i\",\"saw\",\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "[word_index[w] for w in test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymT35BgXEbM7"
      },
      "source": [
        "### Load pre-trained word embeddings\n",
        "Let's download pre-trained GloVe embeddings (a 822M zip file).\n",
        "Rather than training our own word vectors from scratch, we will leverage on GloVe. Its authors have released four text files with word vectors trained on different massive web datasets.\n",
        "\n",
        "The archive contains text-encoded vectors of various sizes: 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n",
        "\n",
        "glove.6B = Wikipedia 2014 + Gigaword 5. It was trained on a corpus of 6 billion tokens and contains a vocabulary of 400 000 tokens.\n",
        "\n",
        "You'll need to run the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "AybfgXLHEimc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are not running on Google Colab\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    colab = True\n",
        "    print ('You are running on Google Colab')\n",
        "else:\n",
        "    colab = False\n",
        "    print ('You are not running on Google Colab')\n",
        "\n",
        "if colab:\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    !ls '/content'\n",
        "    !unzip -q glove.6B.zip\n",
        "    # After unzipping the downloaded file we find 4 txt files: glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt. \n",
        "    # As their filenames suggests, they have vectors with different dimensions.\n",
        "    !ls '/content'\n",
        "    # move glove.6B.100d.txt to My Drive so it will be available for use in the future\n",
        "    !mv '/content/glove.6B.100d.txt' '/content/gdrive/My Drive/glove.6B.100d.txt'\n",
        "\n",
        "# if your are working local you can download the file from http://nlp.stanford.edu/data/glove.6B.zip and unzip it in you datasets directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is4_imN2E2Z3"
      },
      "source": [
        "If we used glove.6B.50d.txt and we printed  the content of the file on console, we could see that each line contain as first element a word followed by 50 real numbers. For instance these are the first two lines, corresponding to tokens \"the\" and \",\":\n",
        "\n",
        "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
        "\n",
        ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
        "\n",
        "Let's make a dict mapping words (strings) to their NumPy vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcp1cFCvFASx",
        "outputId": "11271999-0e84-4499-917b-4da63b609171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# we compute an index mapping words to known embeddings\n",
        "# by parsing the data dump of pre-trained embeddings:\n",
        "\n",
        "if colab: \n",
        "  path_to_glove_file = '/content/gdrive/My Drive/glove.6B.100d.txt'\n",
        "else:\n",
        "  path_to_glove_file = 'datasets/glove.6B.100d.txt'\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(path_to_glove_file, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fo3bId0FUVh"
      },
      "source": [
        "Now, let's prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. It's a simple NumPy matrix where entry at index $i$ is the pre-trained vector for the word of index $i$ in our vectorizer's vocabulary. That means we are mapping words from the vocabulary of our dataset to their corresponding coordinates in the 100 dimensional geometric space from Glove. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrIogMZQFXw9",
        "outputId": "1a137f70-ce23-4322-f8d1-c4751e0cbaef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 5476 words (2455 misses)\n",
            "*** shape of the embedding matrix:***\n",
            "(7933, 100)\n",
            "*** Missed words = words not in word_index ***\n",
            "['', '[UNK]', 'ltgt', 'Ãœ', 'thanx', 'Â£1000', '150ppm', 'aight', 'Â£2000', '150p']\n",
            "\n",
            "*** i has which index in word_index? ***\n",
            "2\n",
            "\n",
            "*** the vector of 100 floats representing i ***\n",
            "[-0.046539    0.61966002  0.56647003 -0.46584001 -1.18900001  0.44599\n",
            "  0.066035    0.31909999  0.14679    -0.22119001  0.79238999  0.29905\n",
            "  0.16073     0.025324    0.18678001 -0.31000999 -0.28108001  0.60514998\n",
            " -1.0654      0.52476001  0.064152    1.03579998 -0.40779001 -0.38011\n",
            "  0.30801001  0.59964001 -0.26991001 -0.76034999  0.94221997 -0.46919\n",
            " -0.18278     0.90652001  0.79671001  0.24824999  0.25713     0.6232\n",
            " -0.44768     0.65357     0.76902002 -0.51229    -0.44332999 -0.21867\n",
            "  0.38370001 -1.14830005 -0.94397998 -0.15062     0.30012    -0.57805997\n",
            "  0.20175    -1.65910006 -0.079195    0.026423    0.22051001  0.99713999\n",
            " -0.57538998 -2.72659993  0.31448001  0.70521998  1.43809998  0.99125999\n",
            "  0.13976     1.34739995 -1.1753      0.0039503   1.02980006  0.064637\n",
            "  0.90886998  0.82871997 -0.47003001 -0.10575     0.5916     -0.42210001\n",
            "  0.57331002 -0.54114002  0.10768     0.39783999 -0.048744    0.064596\n",
            " -0.61436999 -0.28600001  0.50669998 -0.49757999 -0.81569999  0.16407999\n",
            " -1.96300006 -0.26693001 -0.37593001 -0.95846999 -0.85839999 -0.71577001\n",
            " -0.32343    -0.43121001  0.41391999  0.28374001 -0.70931     0.15003\n",
            " -0.2154     -0.37616    -0.032502    0.80620003]\n",
            "\n",
            "*** cat has which index in word_index? ***\n",
            "2136\n",
            "\n",
            "*** the vector of 100 floats representing cat ***\n",
            "[ 0.23088001  0.28283     0.6318     -0.59411001 -0.58599001  0.63255\n",
            "  0.24402    -0.14108001  0.060815   -0.78979999 -0.29102001  0.14286999\n",
            "  0.72273999  0.20428     0.1407      0.98756999  0.52533001  0.097456\n",
            "  0.8822      0.51221001  0.40204     0.21168999 -0.013109   -0.71616\n",
            "  0.55387002  1.14520001 -0.88044    -0.50216001 -0.22814     0.023885\n",
            "  0.1072      0.083739    0.55014998  0.58478999  0.75816     0.45706001\n",
            " -0.28001001  0.25224999  0.68965    -0.60971999  0.19577999  0.044209\n",
            " -0.31136    -0.68826002 -0.22721     0.46184999 -0.77161998  0.10208\n",
            "  0.55636001  0.067417   -0.57207     0.23735     0.47170001  0.82765001\n",
            " -0.29262999 -1.34220004 -0.099277    0.28139001  0.41604     0.10583\n",
            "  0.62203002  0.89495999 -0.23446     0.51349002  0.99378997  1.1846\n",
            " -0.16364001  0.20653     0.73853999  0.24059001 -0.96473002  0.13481\n",
            " -0.0072484   0.33015999 -0.12365     0.27191001 -0.40950999  0.021909\n",
            " -0.60689998  0.40755001  0.19566    -0.41802001  0.18636    -0.032652\n",
            " -0.78570998 -0.13846999  0.044007   -0.084423    0.04911     0.24104001\n",
            "  0.45273    -0.18682     0.46182001  0.089068   -0.18185    -0.01523\n",
            " -0.73680001 -0.14532     0.15104    -0.71493   ]\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(voc) + 2\n",
        "# each word is represented by a vector of 100 floats (glove.6B.100d.txt)\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "missed_words = []\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "# word_index is a dictionary that maps each word to an index\n",
        "# we loop through all the words of word_index.items()\n",
        "# the items() method of a dictionary returns a list of tuples (key, index) for all elements in the dictionary\n",
        "for word, i in word_index.items():\n",
        "# we try to retrieve the vector of 100 floats for this word out of embeddings_index  \n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "# if we found the corresponding vector of 100 floats    \n",
        "    if embedding_vector is not None:\n",
        "      # we put the vector on position i of embedding_matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1    \n",
        "    else:\n",
        "      # Words not found in embedding index will be all-zeros.    \n",
        "        misses += 1\n",
        "        missed_words.append(word)\n",
        "        \n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "print(\"*** shape of the embedding matrix:***\")\n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "print(\"*** Missed words = words not in word_index ***\")\n",
        "print(missed_words[0:10])\n",
        "print()\n",
        "print(\"*** i has which index in word_index? ***\")\n",
        "index_i = word_index['i']\n",
        "print(index_i)\n",
        "print()\n",
        "print(\"*** the vector of 100 floats representing i ***\")\n",
        "print(embedding_matrix[index_i])\n",
        "print()\n",
        "print(\"*** cat has which index in word_index? ***\")\n",
        "index_cat = word_index['cat']\n",
        "print(index_cat)\n",
        "print()\n",
        "print(\"*** the vector of 100 floats representing cat ***\")\n",
        "print(embedding_matrix[index_cat])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aElU90qXVV0I"
      },
      "source": [
        "### Prepare the data\n",
        "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays are right-padded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x07Drl07zwE",
        "outputId": "88ab9376-38ee-4e6f-c413-f920f3db4632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 177   13 1013   42    6 3963    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  54    2   30  108   97    3  455   49  148   44   88 1251   25   40\n",
            "    21 5100  256   30 5875   41  389    0    0    0    0]\n",
            " [   2   31   25   16   30  108  363  268  171    2  252  287   11 1190\n",
            "   217    0    0    0    0    0    0    0    0    0    0]\n",
            " [2497   15  399   73   17   92  191  217    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  54    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]]\n",
            "(3900, 1)\n",
            "(1672, 1)\n",
            "(3900, 25)\n",
            "(1672, 25)\n",
            "----------------\n",
            "[1 1 1 ... 1 1 1]\n",
            "(3900,)\n",
            "(1672,)\n"
          ]
        }
      ],
      "source": [
        "X_train_final = vectorizer(np.array([s for s in X_train])).numpy()\n",
        "X_test_final = vectorizer(np.array([s for s in X_test])).numpy()\n",
        "\n",
        "y_train_final = np.array(y_train)\n",
        "y_test_final = np.array(y_test)\n",
        "\n",
        "print(X_train_final[:5])\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_train_final.shape)\n",
        "print(X_test_final.shape)\n",
        "print('----------------')\n",
        "print(y_train_final)\n",
        "print(y_train_final.shape)\n",
        "print(y_test_final.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build the model\n",
        "- Keras provides many preprocessing layers. We have seen earlier the `Flatten()` layer as an example.  \n",
        "- Another exampele is the `Embedding()` layer.  \n",
        "- It turns positive integers (indexes) into dense vectors of fixed size.  \n",
        "        e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
        "- We use it to load the pre-trained word embeddings matrix into an Embedding layer.\n",
        "- All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index in the embedding matrix, so it will map for each sms message the index of each word to the coordinates in the geometry.\n",
        "- The `keras.layers.Embedding` layer handles an embedding matrix, which is trainable by default. \n",
        "- We need to set `trainable=False` so as to keep the embeddings fixed (we don't want to update them during training because we use pretrained word embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "\n",
        "def initial_model():\n",
        "    # we create a variable called model, and we set it equal to an instance of a Sequential object.\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Embedding(input_dim=num_tokens, output_dim=embedding_dim, input_length=25,embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False))\n",
        "    model.add(keras.layers.Flatten(input_shape=[num_tokens, embedding_dim]))\n",
        "\n",
        "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "    # Before we can train our model, we must compile it\n",
        "    # To the compile() function, we are passing the optimizer, the loss function, and the metrics that we would like to see. \n",
        "    # Notice that the optimizer we have specified is called Adam. Adam is just a variant of SGD. \n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer= tf.keras.optimizers.Adam(learning_rate = 0.01),\n",
        "                  metrics=['accuracy']) \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpGrdwWQ85iu",
        "outputId": "f889e35b-ab9f-417f-a753-17596650fcb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 25, 100)           793300    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2500)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               250100    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 202       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,073,902\n",
            "Trainable params: 280,602\n",
            "Non-trainable params: 793,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_1 = initial_model()\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "61/61 [==============================] - 1s 5ms/step - loss: 0.7072 - accuracy: 0.8515 - val_loss: 0.1793 - val_accuracy: 0.9193\n",
            "Epoch 2/7\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9372 - val_loss: 0.1266 - val_accuracy: 0.9569\n",
            "Epoch 3/7\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.1315 - accuracy: 0.9572 - val_loss: 0.1636 - val_accuracy: 0.9342\n",
            "Epoch 4/7\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.1140 - accuracy: 0.9654 - val_loss: 0.1085 - val_accuracy: 0.9635\n",
            "Epoch 5/7\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.9726 - val_loss: 0.1893 - val_accuracy: 0.9486\n",
            "Epoch 6/7\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9746 - val_loss: 0.0916 - val_accuracy: 0.9707\n",
            "Epoch 7/7\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9795 - val_loss: 0.0949 - val_accuracy: 0.9701\n"
          ]
        }
      ],
      "source": [
        "# We now add batch size to the mix of training parameters\n",
        "# If you don't specify batch size below, all training data will be used for each learning step\n",
        "batch_size = 64\n",
        "epochs = 7\n",
        "\n",
        "history_1 = model_1.fit(X_train_final, y_train_final,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_test_final, y_test_final)\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "nCkT4JxmfP73",
        "outputId": "b1f680f1-62f7-46a8-d7da-2bde2fbb5c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set Accuracy:   0.99\n",
            "Training set Loss: 0.0154\n",
            "\n",
            "Validation set Accuracy:   0.97\n",
            "Validation set Loss: 0.0949\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# model_1 now contains the model at the end of the training run\n",
        "# We analyse the result:\n",
        "\n",
        "[train_loss, train_accuracy] = model_1.evaluate(X_train_final, y_train_final, verbose=0)\n",
        "print(\"Training set Accuracy:{:7.2f}\".format(train_accuracy))\n",
        "print(\"Training set Loss:{:7.4f}\\n\".format(train_loss))\n",
        "\n",
        "[val_loss, val_accuracy] = model_1.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "print(\"Validation set Accuracy:{:7.2f}\".format(val_accuracy))\n",
        "print(\"Validation set Loss:{:7.4f}\\n\".format(val_loss))\n",
        "\n",
        "#Now we visualise what happened during training\n",
        "plot_history(history_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fACX0ULsgDjG",
        "outputId": "0ffc207f-d3b6-46b1-ccad-ae005b65889d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[9.9932683e-01 6.7321205e-04]]\n"
          ]
        }
      ],
      "source": [
        "X_example = vectorizer(np.array([s for s in [\"URGENT! Your Mobile No. was awarded a â‚¬2000 Bonus Caller Prize\"]])).numpy()\n",
        "pred = model_1.predict([X_example])\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbMA5Svgg6od",
        "outputId": "2c3693b6-eb1f-4042-a2e7-3b1aecb35c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.168667e-05 9.999883e-01]]\n"
          ]
        }
      ],
      "source": [
        "X_example = vectorizer(np.array([s for s in [\"Please call me\"]])).numpy()\n",
        "pred = model_1.predict([X_example])\n",
        "print(pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Kopie van Using pretrained word embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
